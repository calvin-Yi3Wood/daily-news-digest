"""
æ¯æ—¥èµ„è®¯æ¨é€ç³»ç»Ÿ - ä¸»ç¨‹åº
Daily News Digest System

åŠŸèƒ½:
- æ¯å¤©æ—©ä¸Š8ç‚¹ï¼ˆåŒ—äº¬æ—¶é—´ï¼‰è‡ªåŠ¨è¿è¡Œ
- ä½¿ç”¨GLM 4.6æœç´¢æœ€æ–°èµ„è®¯
- è·å–GitHubçƒ­é—¨é¡¹ç›®
- æ™ºèƒ½å»é‡
- æ¨é€åˆ°å¾®ä¿¡ï¼ˆä¸»ï¼‰æˆ–é‚®ç®±ï¼ˆå¤‡ï¼‰
"""

import os
import sys
import yaml
import logging
from datetime import datetime
from typing import List, Dict
from pathlib import Path

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv()

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / 'src'))

from collectors import GLMSearchCollector, GitHubTrendingCollector, ContentDeduplicator, RSSCollector
from formatters import MarkdownFormatter
from pushers import WeChatWebhookPusher, EmailSender
from processors import ContentProcessor

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/daily_news_digest.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)


def load_config(config_path: str = 'config/config.yaml') -> Dict:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def load_keywords(keywords_path: str = 'config/keywords.yaml') -> List[Dict]:
    """
    åŠ è½½æœç´¢å…³é”®è¯ï¼ˆæ”¯æŒæ¯ä¸ªå…³é”®è¯ä¸åŒçš„ç»“æœæ•°ï¼‰

    Returns:
        List[Dict]: åŒ…å«queryå’Œmax_resultsçš„å­—å…¸åˆ—è¡¨
        ä¾‹å¦‚: [{'query': 'xxx', 'max_results': 10}, ...]
    """
    with open(keywords_path, 'r', encoding='utf-8') as f:
        data = yaml.safe_load(f)

    keywords = []
    for category in data.get('categories', []):
        if category.get('enabled', True):
            # è·å–è¯¥åˆ†ç±»çš„search_params
            search_params = category.get('search_params', {})
            count = search_params.get('count', 10)  # é»˜è®¤10

            for keyword_item in category.get('keywords', []):
                if isinstance(keyword_item, dict):
                    query = keyword_item.get('query', '')
                else:
                    query = keyword_item
                if query:
                    keywords.append({
                        'query': query,
                        'max_results': count,
                        'category': category.get('name', 'æœªåˆ†ç±»')
                    })

    logger.info(f"åŠ è½½äº†{len(keywords)}ä¸ªæœç´¢å…³é”®è¯")
    return keywords


def collect_news(config: Dict) -> tuple:
    """
    æ”¶é›†æ–°é—»èµ„è®¯

    Returns:
        (rss_results, github_projects) å…ƒç»„
    """
    logger.info("=== å¼€å§‹æ”¶é›†èµ„è®¯ ===")

    # 1. RSSæ–°é—»æ”¶é›†ï¼ˆæ›¿ä»£GLMæœç´¢ï¼Œæ—¶é—´100%å¯é ï¼‰
    logger.info("æ­¥éª¤1: RSSæ–°é—»æ”¶é›†")
    rss_collector = RSSCollector()

    # æ”¶é›†æœ€è¿‘24å°æ—¶çš„RSSæ–‡ç« 
    articles = rss_collector.collect(hours=24, max_per_feed=10)

    # æ ¼å¼åŒ–ä¸ºGLMå¤„ç†å™¨å…¼å®¹çš„æ ¼å¼
    rss_results = rss_collector.format_for_glm(articles)

    logger.info(f"RSSæ”¶é›†å®Œæˆ: {len(articles)}ç¯‡æ–‡ç« , {len(rss_results)}ä¸ªåˆ†ç±»")

    # 2. GitHubè¶‹åŠ¿
    logger.info("æ­¥éª¤2: GitHubè¶‹åŠ¿é¡¹ç›®")
    github_collector = GitHubTrendingCollector()
    github_config = config.get('github', {})

    languages = github_config.get('languages', ['Python', 'JavaScript', 'TypeScript'])
    days = github_config.get('trending_days', 7)
    top_n = github_config.get('top_n', 10)

    # è·å–æ‰€æœ‰è¯­è¨€çš„é¡¹ç›®å¹¶åˆå¹¶
    all_projects = []
    for language in languages:
        projects = github_collector.get_trending(language, days, top_n // len(languages))
        all_projects.extend(projects)

    # æŒ‰æ˜Ÿæ ‡æ’åº
    all_projects.sort(key=lambda x: x.get('stars', 0), reverse=True)

    # åªä¿ç•™å‰top_nä¸ª
    github_projects = all_projects[:top_n]

    logger.info(f"æ”¶é›†å®Œæˆ: RSSæ–‡ç« {len(articles)}ç¯‡, GitHubé¡¹ç›®{len(github_projects)}ä¸ª")
    return rss_results, github_projects


def deduplicate_content(glm_results: List[Dict], github_projects: List[Dict], config: Dict):
    """
    å»é‡å¤„ç†

    Returns:
        å»é‡åçš„ç»“æœ
    """
    logger.info("=== å¼€å§‹å»é‡å¤„ç† ===")

    dedup_config = config.get('deduplication', {})
    threshold = dedup_config.get('similarity_threshold', 0.8)

    deduplicator = ContentDeduplicator(similarity_threshold=threshold)

    # GitHubé¡¹ç›®å»é‡ï¼ˆåŸºäºURLï¼‰
    github_items = []
    for project in github_projects:
        github_items.append({
            'title': project.get('full_name', ''),
            'url': project.get('url', ''),
            'data': project
        })

    unique_github_items = deduplicator.deduplicate(github_items)
    unique_github_projects = [item['data'] for item in unique_github_items]

    logger.info(f"å»é‡å®Œæˆ: GitHubé¡¹ç›® {len(github_projects)} -> {len(unique_github_projects)}")
    return glm_results, unique_github_projects


def intelligent_process(glm_results: List[Dict], github_projects: List[Dict]) -> Dict:
    """
    æ™ºèƒ½å¤„ç†å†…å®¹ï¼ˆä½¿ç”¨GLMå¤§æ¨¡å‹è¿›è¡ŒäºŒæ¬¡å¤„ç†ï¼‰

    Args:
        glm_results: GLMæœç´¢ç»“æœ
        github_projects: GitHubé¡¹ç›®åˆ—è¡¨

    Returns:
        å¤„ç†åçš„å†…å®¹å­—å…¸
    """
    logger.info("=== å¼€å§‹æ™ºèƒ½å†…å®¹å¤„ç† ===")

    try:
        processor = ContentProcessor()
        processed = processor.process_news(glm_results, github_projects)

        if processed.get('success'):
            logger.info(f"âœ… æ™ºèƒ½å¤„ç†æˆåŠŸï¼è¾“å‡º{processed.get('char_count', 0)}å­—ç¬¦")
            return processed
        else:
            logger.error(f"âŒ æ™ºèƒ½å¤„ç†å¤±è´¥: {processed.get('error', 'æœªçŸ¥é”™è¯¯')}")
            return processed

    except Exception as e:
        logger.error(f"æ™ºèƒ½å¤„ç†å¼‚å¸¸: {str(e)}", exc_info=True)
        return {
            'success': False,
            'content': '',
            'error': str(e)
        }


def format_and_push_processed(processed_content: Dict, config: Dict) -> bool:
    """
    æ¨é€æ™ºèƒ½å¤„ç†åçš„å†…å®¹

    Args:
        processed_content: æ™ºèƒ½å¤„ç†åçš„å†…å®¹å­—å…¸
        config: ç³»ç»Ÿé…ç½®

    Returns:
        Trueè¡¨ç¤ºæˆåŠŸï¼ŒFalseè¡¨ç¤ºå¤±è´¥
    """
    logger.info("=== å¼€å§‹æ¨é€å¤„ç†åçš„å†…å®¹ ===")

    if not processed_content.get('success'):
        logger.error("å†…å®¹å¤„ç†å¤±è´¥ï¼Œæ— æ³•æ¨é€")
        return False

    markdown = processed_content.get('content', '')
    if not markdown:
        logger.error("å¤„ç†åçš„å†…å®¹ä¸ºç©º")
        return False

    # æ·»åŠ ç§‘æ¯”åè¨€ï¼ˆåœ¨åˆ†å‰²å‰ï¼‰
    formatter = MarkdownFormatter()
    kobe_quote = formatter.get_random_kobe_quote()
    if kobe_quote:
        # æ·»åŠ ç§‘æ¯”åè¨€åˆ°å†…å®¹æœ«å°¾
        kobe_lines = ["\n\n---\n"]
        kobe_lines.append("## ğŸ€ ä»Šæ—¥åè¨€ - Kobe Bryant\n")

        if kobe_quote.get('show_category'):
            kobe_lines.append(f"*{kobe_quote['category']}*\n")

        format_type = kobe_quote.get('format', 'bilingual')
        if format_type == 'bilingual':
            kobe_lines.append(f"> **{kobe_quote['en']}**\n")
            kobe_lines.append(f"> **{kobe_quote['zh']}**\n")
        elif format_type == 'en_only':
            kobe_lines.append(f"> {kobe_quote['en']}\n")
        elif format_type == 'zh_only':
            kobe_lines.append(f"> {kobe_quote['zh']}\n")

        markdown += ''.join(kobe_lines)
        logger.info("âœ… å·²æ·»åŠ ç§‘æ¯”åè¨€")

    # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†å‰²
    max_size = config.get('formatter', {}).get('max_chunk_size', 1300)
    chunks = formatter.split_content(markdown, max_size)

    logger.info(f"å†…å®¹åˆ†å‰²å®Œæˆï¼Œå…±{len(chunks)}ä¸ªéƒ¨åˆ†")

    # æ¨é€
    push_config = config.get('push_strategy', {})
    primary = push_config.get('primary', 'wechat')
    enable_fallback = push_config.get('enable_fallback', True)

    success = False

    # ä¸»æ¨é€æ–¹å¼
    if primary == 'wechat':
        logger.info("ä½¿ç”¨å¾®ä¿¡Webhookæ¨é€")
        try:
            pusher = WeChatWebhookPusher()
            success = pusher.send_in_chunks(chunks) if len(chunks) > 1 else pusher.send_markdown(chunks[0])
        except Exception as e:
            logger.error(f"å¾®ä¿¡æ¨é€å¤±è´¥: {str(e)}")
            success = False

    elif primary == 'email':
        logger.info("ä½¿ç”¨é‚®ç®±æ¨é€")
        try:
            sender = EmailSender()
            success = sender.send_markdown_as_html(markdown)
        except Exception as e:
            logger.error(f"é‚®ç®±æ¨é€å¤±è´¥: {str(e)}")
            success = False

    # å¤‡ç”¨æ¨é€
    if not success and enable_fallback:
        fallback = push_config.get('fallback', 'email')
        logger.warning(f"ä¸»æ¨é€å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ¨é€: {fallback}")

        if fallback == 'email':
            try:
                sender = EmailSender()
                success = sender.send_markdown_as_html(markdown)
            except Exception as e:
                logger.error(f"å¤‡ç”¨é‚®ç®±æ¨é€å¤±è´¥: {str(e)}")
                success = False

    return success


def format_and_push(glm_results: List[Dict], github_projects: List[Dict], config: Dict) -> bool:
    """
    æ ¼å¼åŒ–å¹¶æ¨é€ï¼ˆä¿ç•™æ—§ç‰ˆæœ¬ä½œä¸ºå¤‡ç”¨ï¼‰

    Returns:
        Trueè¡¨ç¤ºæˆåŠŸï¼ŒFalseè¡¨ç¤ºå¤±è´¥
    """
    logger.info("=== å¼€å§‹æ ¼å¼åŒ–å’Œæ¨é€ ===")

    # 1. æ ¼å¼åŒ–
    formatter = MarkdownFormatter()
    markdown = formatter.format_digest(glm_results, github_projects)

    # 2. æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†å‰²
    max_size = config.get('formatter', {}).get('max_chunk_size', 1300)
    chunks = formatter.split_content(markdown, max_size)

    logger.info(f"å†…å®¹æ ¼å¼åŒ–å®Œæˆï¼Œå…±{len(chunks)}ä¸ªéƒ¨åˆ†")

    # 3. æ¨é€
    push_config = config.get('push_strategy', {})
    primary = push_config.get('primary', 'wechat')
    enable_fallback = push_config.get('enable_fallback', True)

    success = False

    # ä¸»æ¨é€æ–¹å¼
    if primary == 'wechat':
        logger.info("ä½¿ç”¨å¾®ä¿¡Webhookæ¨é€ï¼ˆä¸»æ¨é€ï¼‰")
        try:
            pusher = WeChatWebhookPusher()
            success = pusher.send_in_chunks(chunks) if len(chunks) > 1 else pusher.send_markdown(chunks[0])
        except Exception as e:
            logger.error(f"å¾®ä¿¡æ¨é€å¤±è´¥: {str(e)}")
            success = False

    elif primary == 'email':
        logger.info("ä½¿ç”¨é‚®ç®±æ¨é€ï¼ˆä¸»æ¨é€ï¼‰")
        try:
            sender = EmailSender()
            success = sender.send_markdown_as_html(markdown)
        except Exception as e:
            logger.error(f"é‚®ç®±æ¨é€å¤±è´¥: {str(e)}")
            success = False

    # å¤‡ç”¨æ¨é€
    if not success and enable_fallback:
        fallback = push_config.get('fallback', 'email')
        logger.warning(f"ä¸»æ¨é€å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ¨é€: {fallback}")

        if fallback == 'email':
            try:
                sender = EmailSender()
                success = sender.send_markdown_as_html(markdown)
            except Exception as e:
                logger.error(f"å¤‡ç”¨é‚®ç®±æ¨é€å¤±è´¥: {str(e)}")
                success = False

    return success


def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 60)
    logger.info("æ¯æ—¥èµ„è®¯æ¨é€ç³»ç»Ÿå¯åŠ¨")
    logger.info(f"è¿è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("=" * 60)

    try:
        # åŠ è½½é…ç½®
        config = load_config()

        # æ”¶é›†èµ„è®¯
        glm_results, github_projects = collect_news(config)

        # å»é‡å¤„ç†
        glm_results, github_projects = deduplicate_content(glm_results, github_projects, config)

        # ğŸ†• æ™ºèƒ½å†…å®¹å¤„ç†ï¼ˆä½¿ç”¨GLMå¤§æ¨¡å‹è¿›è¡ŒäºŒæ¬¡å¤„ç†ï¼‰
        processed_content = intelligent_process(glm_results, github_projects)

        # æ¨é€å¤„ç†åçš„å†…å®¹
        success = format_and_push_processed(processed_content, config)

        if success:
            logger.info("âœ… èµ„è®¯æ¨é€æˆåŠŸï¼")
            return 0
        else:
            logger.error("âŒ èµ„è®¯æ¨é€å¤±è´¥ï¼")
            return 1

    except Exception as e:
        logger.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}", exc_info=True)
        return 1

    finally:
        logger.info("=" * 60)
        logger.info("ç¨‹åºè¿è¡Œç»“æŸ")
        logger.info("=" * 60)


if __name__ == '__main__':
    # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
    os.makedirs('logs', exist_ok=True)

    # è¿è¡Œä¸»ç¨‹åº
    exit_code = main()
    sys.exit(exit_code)
